---
title: "PREDICTING THE PROPOSAL: A Data-Driven Approach to Love"
author: "The Extrapolats"
date: "`r Sys.Date()`"
output:  
    rmdformats::readthedown:
      toc_float: true
      toc_depth: 3
      number_sections: false
      code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = F, results = "show", message = F)
library(ezids)
```

# ABSTRACT
```{r}
```

# INTRODUCTION
```{r}
```

# SUMMARY OF THE DATASET
```{r}
In our analysis of marriage proposals, a comprehensive dataset is used-one that provides a data-driven perspective on relationship dynamics and proposal outcomes. The set consists of 10,000 unique observations, each of which has been measured over 9 preselected variables indicative of both demographic information and relationship quality metrics. This data is particularly unique in that it contains no missing values and no outliers, thus rendering it perfect for robust statistical analysis.

The dataset bridges the gap between the quantifiable aspects of the relationships, such as income and physical distance, with the more nuanced elements that are the compatibility and communication scores. This dual approach gives a unique opportunity to understand how both tangible and intangible factors influence the success of marriage proposals in modern society.

```

# DATASET OVERVIEW
```{r}
The architecture of the variables in our dataset reflects the same thoughtfulness for the complexity of relationship dynamics. For data organization, there are mainly two categories: integer-type variables and categorical measurements. The integer-type variables include basic measures such as height in centimeters, age in years, income in USD, and the physical distance in kilometers between partners. These give concrete, measurable aspects of the relationship dynamic. 
Complementing these numerical measures are five categorical variables, which capture the more subtle aspects of relationships. These include the RomanticGestureScore, CompatibilityScore, and CommunicationScore, each on a scale ranging from 0 to 10. The dataset also includes a binary Response variable indicating either proposal acceptance or rejection, with an AgeCategory classification that further segments the population into Young, Middle-aged, and Senior groups.
Its completeness and consistency further point to the dataset being of exceptional quality, since there are no values missing, no duplicate rows, and the data type for each column is well defined. It provides a sound basis for statistical analysis and applications involving machine learning. Balanced distributions across categories provide assurance that the samples are representative of findings generalizable to reality.
This dataset has a myriad of uses in relationship studies, ranging from the detailed analysis of factors that influence proposal success to the development of predictive models for proposal outcomes and demographic research in modern relationships. The comprehensiveness of the data, coupled with its quality, makes it invaluable for the understanding of the convolutions of marriage proposals in contemporary society.

```

## Loading the Dataset 


### Before Cleaning

```{r}
# Reading the dataset
proposal_df = read.csv("marriage_proposal.csv")
```

```{r}
# Structure of the dataset
str(proposal_df)
```

# DATA CLEANING

```{r}

# Check for empty and NA values 
empty_values <- sapply(proposal_df, function(x) sum(x == "", na.rm = TRUE))
na_values <- sapply(proposal_df, function(x) sum(is.na(x)))
print("Empty Values in Each Column:")
print(empty_values)
print("NA Values in Each Column:")
print(na_values)

```

```{r}

# Converting required 'int' columns into 'factor'
proposal_df$RomanticGestureScore <- as.factor(proposal_df$RomanticGestureScore)
proposal_df$CompatibilityScore <- as.factor(proposal_df$CompatibilityScore)
proposal_df$CommunicationScore <- as.factor(proposal_df$CommunicationScore)
proposal_df$Response <- as.factor(proposal_df$Response)

```



## Summary of the cleaned Dataset

```{r}

print("Summary of cleaned data")
summary(proposal_df)

```

# EXPLORATORY DATA ANALYSIS
### Loading necessary libraries

```{r}
# Load necessary libraries
library(ggplot2)
library(dplyr)
library(psych)
library(tidyverse) 
library(ggplot2) 
library(viridis) 
library(RColorBrewer) 
library(ggmosaic)

library(gridExtra)
library(caret)
library(pROC)
library(rpart)
library(rpart.plot)
library(randomForest)
```


### Distribution of the Numeric variables

Let’s take a look at descriptive stats of Numeric variables.
```{r}
# Descriptive Statistics
# Numeric variables
describe(proposal_df %>% select(Age, Height, Income, DistanceKM))
```

Graphical Representations of Integer variables.

```{r}
# Histogram of age

ggplot(proposal_df, aes(x = Age, fill = Age)) + geom_histogram(binwidth = 5, fill = "lightpink", color = "black") + labs(title = "Age Distribution", x = "Age", y = "Frequency") + theme_minimal()
```

```{r, warning=FALSE}
# Frequency polygon for Height
ggplot(proposal_df, aes(x = Height)) + 
  geom_freqpoly(binwidth = 5, color = "grey", size = 1.2) + 
  labs(title = "Height Distribution", x = "Height (cm)", y = "Frequency") + 
  theme_minimal()

```


```{r}
# We can use this function to check if any outliers were detected in variables.
#proposal_df <- outlierKD2(proposal_df, DistanceKM, rm = TRUE, boxplt = TRUE, histogram = FALSE, qqplt = TRUE)
```

```{r}
ggplot(proposal_df, aes(x = cut(Income, breaks = c(5000, 20000, 30000, 40000, 50000), labels = c("Low Income", "Medium Income", "High Income", "Very High Income"), include.lowest = TRUE))) + geom_bar(fill = "steelblue", color = "black") + labs(title = "Population Distribution by Income Category", x = "Income Category", y = "Frequency") + theme_minimal()

```

```{r}
# Boxplot for Distance
ggplot(proposal_df, aes(y = DistanceKM)) + geom_boxplot(fill = "lightyellow", color = "black") + labs(title = "Distance (KM) Boxplot", y = "Distance (KM)") + theme_minimal()
```

Graphical Representations of Factorial variables.

```{r, warning=FALSE}
# Pie chart for Response variable
ggplot(proposal_df, aes(x = "", fill = Response)) + geom_bar(width = 1, stat = "count", color = "black") + coord_polar(theta = "y") + geom_text(aes(label = scales::percent(..count../sum(..count..))), position = position_stack(vjust = 0.5), stat = "count") + scale_fill_manual(values = c("0" = "red", "1" = "green"), labels = c("Rejected", "Accepted")) + labs(title = "Proposal Response Distribution", fill = "Response") + theme_void() + theme(plot.title = element_text(hjust = 0.5, size = 16))
```

```{r}
# bar plot for Communication Score
p1 <- ggplot(proposal_df, aes(x = as.factor(CommunicationScore))) + geom_bar(fill = "blue", color = "black") + labs(title = "Communication Score Distribution", x = "Communication Score", y = "Frequency") + theme_minimal()

# bar plot for Compatibility Score
p2 <- ggplot(proposal_df, aes(x = as.factor(CompatibilityScore))) + geom_bar(fill = "green", color = "black") + labs(title = "Compatibility Score Distribution", x = "Compatibility Score", y = "Frequency") + theme_minimal()

grid.arrange(p1, p2, ncol = 2)
```

The exploratory analysis showed low correlations among individual attributes such as Income, Age, and DistanceKM concerning proposal acceptance. We observed a minor rise in acceptance rates among higher-income groups, yet anomalies in Income and DistanceKM indicate that refining or modifying the data may improve model effectiveness. The distribution of Age indicated that younger individuals, particularly those aged 25–35, are more represented, with minor declines in acceptance rates in older groups. Distance played a critical role, with shorter distances correlating positively with acceptance, though its influence diminished significantly beyond 50 KM. Overall, the dataset lacked strong linear relationships, indicating that advanced feature engineering and interaction terms may be necessary to capture underlying patterns.

### Outlier removal
```{r}
```

# SMART QUESTIONS
```{r}
```

### Question 1:
```{r}
```

#### OBSERVATION:
```{r}
```

#### RESULT:
```{r}
```

### Question 2: Can a linear regression model effectively predict the Income of an individual using Age, Height, and RomanticGestureScore as predictors?

```{r}
# linear regression model
model_lr <- lm(Income ~ Age + Height + RomanticGestureScore, data = proposal_df)

# Summary of the model
summary(model_lr)

```

#### OBSERVATION:
```{r}
# R-squared value
rsq <- summary(model_lr)$r.squared
cat("R-squared:", rsq, "\n")

# Mean Squared Error (MSE)
mse <- mean(residuals(model_lr)^2)
cat("Mean Squared Error:", mse, "\n")
```

R-Squared value and MSE were very low for this model.

```{r}
# Plot diagnostic plots
par(mfrow = c(2, 2))
plot(model_lr)
```

We will now try to predict income of an individual who are 30 years old and 160 cm tall with a score of 8 for Romantic Gesture.

#### RESULT:
```{r}
# Sample input data 
input1 <- data.frame(Age = 30, Height = 160, RomanticGestureScore = factor(8, levels = levels(proposal_df$RomanticGestureScore)))

#Prediction model
predict1 <- predict(model_lr, newdata = input1)
predict1
```

The Linear Regression Model has predicted the income of such individual mentioned above to be $26637.

### Question 3: Which classification model (logistic regression, decision tree, or random forest) is the most suitable for predicting whether a proposal will be accepted or rejected based on Relational(RomanticGestureScore, CompatibilityScore, and DistanceKM) features? 

The reason for selecting the three features, RomanticGestureScore, CompatibilityScore, and DistanceKM are due to their role on relational dynamics. RomanticGestureScore represents an individual’s capacity for expressing their affection physically. CompatibilityScore highlights the match of personality traits and key elements in determining the relation's success. DistanceKM on the other hand, represent the physical distance between the partners. Combined together, these features may provide a very valuble insights on the proposal's outcome. So, let's see if the emotional factors play a significant role in determining a relation's success.

First, let's proceed with Logistic Regression Model first. 
```{r, warning=FALSE}
# Split the dataset into training and testing sets 
set.seed(123)  
index <- createDataPartition(proposal_df$Response, p = 0.8, list = FALSE)
train_data <- proposal_df[index, ]
test_data <- proposal_df[-index, ]

# Logistic Regression Model
logistic_model <- glm(Response ~ RomanticGestureScore + CompatibilityScore + CommunicationScore, data = train_data, family = binomial)
logistic_probabilities <- predict(logistic_model, test_data, type = "response")
logistic_predicted <- ifelse(logistic_probabilities > 0.5, 1, 0)
actual <- as.factor(test_data$Response)
predicted <- as.factor(logistic_predicted)

# Confusion Matrix and metrics
confusion <- confusionMatrix(predicted, actual, positive = "1")
accuracy <- confusion$overall["Accuracy"]
precision <- confusion$byClass["Pos Pred Value"]  
recall <- confusion$byClass["Sensitivity"]       
f1_score <- 2 * (precision * recall) / (precision + recall)
# AUC
roc_curve <- roc(actual, logistic_probabilities, levels = c("0", "1"))
auc_value <- auc(roc_curve)

# Print metrics
cat("Logistic Regression Performance Metrics:\n")
cat("Accuracy: ", accuracy, "\n")
cat("Precision: ", precision, "\n")
cat("Recall: ", recall, "\n")
cat("F1-Score: ", f1_score, "\n")
cat("AUC: ", auc_value, "\n")

# Plot ROC Curve 
plot(roc_curve, main = "ROC Curve for Logistic Regression", col = "blue", lwd = 2)
abline(a = 0, b = 1, col = "gray", lty = 2)
auc_text <- paste("AUC =", round(auc_value, 3)) # Add AUC value to the plot
text(0.6, 0.2, auc_text, col = "blue", cex = 1.2)
```

#### OBSERVATION:
The Model uses 80% of the data for training, and the remaining 20% of the data for testing evaluation. The model predicts the outcome of the proposal with probabilities calculated for classification based the three features we have given. We will use few metrics to decide how well the model is performing.

First, we got accuracy as 79.5%, precision as 83.4%, recall as 91.7%, and F1-score as 87.3%. While the balanced F1-score shows the overall reliability of the model, a high recall show the strong sensitivity to the accepted proposals. The AUC value is 81.5%, along with the ROC curve, certifies the model's efficiency.


Now, let's proceed to the next model Decision tree:
```{r, warning=FALSE}
# Split the dataset into training and testing sets 
set.seed(123)
trainIndex <- createDataPartition(proposal_df$Response, p = 0.8, list = FALSE)
trainData <- proposal_df[trainIndex, ]
testData <- proposal_df[-trainIndex, ]

# Decision Tree Model
decision_tree_model <- rpart(Response ~ RomanticGestureScore + CompatibilityScore + CommunicationScore, data = trainData, method = "class")
predicted_probabilities_tree <- predict(decision_tree_model, proposal_df, type = "prob")[, 2]  
predicted_classes_tree <- predict(decision_tree_model, proposal_df, type = "class")            
actual <- as.factor(proposal_df$Response)

# Visualize the Decision Tree
# rpart.plot(decision_tree_model, type = 2, extra = 104, main = "Decision Tree")

# Confusion Matrix and metrics
confusion_tree <- confusionMatrix(as.factor(predicted_classes_tree), actual, positive = "1")
accuracy_tree <- confusion_tree$overall["Accuracy"]
precision_tree <- confusion_tree$byClass["Pos Pred Value"]  
recall_tree <- confusion_tree$byClass["Sensitivity"]        
f1_score_tree <- 2 * (precision_tree * recall_tree) / (precision_tree + recall_tree)
roc_curve_tree <- roc(actual, predicted_probabilities_tree, levels = c("0", "1"))
auc_value_tree <- auc(roc_curve_tree)

# Print Metrics
cat("Decision Tree Metrics:\n")
cat("Accuracy:", round(accuracy_tree, 3), "\n")
cat("Precision:", round(precision_tree, 3), "\n")
cat("Recall:", round(recall_tree, 3), "\n")
cat("F1-Score:", round(f1_score_tree, 3), "\n")
cat("AUC:", round(auc_value_tree, 3), "\n")

# Plot ROC Curve 
plot(roc_curve_tree, main = "ROC Curve for Decision Tree", col = "blue", lwd = 2)
abline(a = 0, b = 1, col = "gray", lty = 2)
auc_text_tree <- paste("AUC =", round(auc_value_tree, 3)) # Add AUC value to the plot
text(0.6, 0.2, auc_text_tree, col = "blue", cex = 1.2)

```
#### OBSERVATION:

In decision tree, we follow the similar approach as Logistic regression to divide the data for training and testing. But unlike Logistic regression, decision tree classifies responses and produces probabilities and classes for evaluation.

Overall, the Decision tree was little under performed compared to the logistic regression. The metrics came out to be 78.6% for accuracy, 81.8% for precision and 87% for F1-score. Although it got a better F1-score (93%), It got a really low AUC (71.7%) and the ROC curve seems to be less curvy compared the previous graph.


Now, let's move on to the last model, Random Forest, Theoretically, this would be the best model out of all three of them. It combines multiple trees to reduce overfitting and leverages random feature selection and bagging, making it more accurate and robust. 
```{r, warning=FALSE}
# Split the dataset into training and testing sets 
set.seed(123)
trainIndex <- createDataPartition(proposal_df$Response, p = 0.8, list = FALSE)
trainData <- proposal_df[trainIndex, ]
testData <- proposal_df[-trainIndex, ]

# Random Forest Model
random_forest_model <- randomForest(Response ~ RomanticGestureScore + CompatibilityScore + CommunicationScore, data = trainData, ntree = 500, mtry = 2, importance = TRUE)

# Print model summary
# print(random_forest_model)

# Predictions
predicted_probabilities_rf <- predict(random_forest_model, proposal_df, type = "prob")[, 2]  
predicted_classes_rf <- predict(random_forest_model, proposal_df, type = "response")         
actual <- as.factor(proposal_df$Response)

# Confusion Matrix and metrics
confusion_rf <- confusionMatrix(as.factor(predicted_classes_rf), actual, positive = "1")
accuracy_rf <- confusion_rf$overall["Accuracy"]
precision_rf <- confusion_rf$byClass["Pos Pred Value"]  
recall_rf <- confusion_rf$byClass["Sensitivity"]        
f1_score_rf <- 2 * (precision_rf * recall_rf) / (precision_rf + recall_rf)
roc_curve_rf <- roc(actual, predicted_probabilities_rf, levels = c("0", "1"))
auc_value_rf <- auc(roc_curve_rf)

# Print Metrics
cat("Random Forest Metrics:\n")
cat("Accuracy:", round(accuracy_rf, 3), "\n")
cat("Precision:", round(precision_rf, 3), "\n")
cat("Recall:", round(recall_rf, 3), "\n")
cat("F1-Score:", round(f1_score_rf, 3), "\n")
cat("AUC:", round(auc_value_rf, 3), "\n")

# Plot ROC Curve 
plot(roc_curve_rf, main = "ROC Curve for Random Forest", col = "blue", lwd = 2)
abline(a = 0, b = 1, col = "gray", lty = 2)
auc_text_rf <- paste("AUC =", round(auc_value_rf, 3)) # Add AUC value to the plot
text(0.6, 0.2, auc_text_rf, col = "blue", cex = 1.2)

# Variable Importance Plot
varImpPlot(random_forest_model, main = "Variable Importance - Random Forest")

```
#### OBSERVATION:

As expected, Random Forest out performed the remaining two models with an accuracy of 80.7%, precision of 84.1%, recall of 92.4% and 88% F1-score. It even has a high AUC score of 84.6% and the ROC curve being more curvy and closer to the left top side indicates how efficient the model is. These improvement using the random forest only implies the model's advantage in reducing the overfitting through learning and capturing complex relationships in the data. 

#### RESULT:
In comparing the three models Logistic Regression, Decision Tree, and Random Forest, the random forest turned out to be most effective model. Logistic regression showed solid potential with 81.5% AUC but not a better model overall. The Decision tree has improved in terms of recall, but the overfit of the data was evident from the low AUC score 71.7%. However, Random forest, with a balanced precision and recall, achieved the highest AUC and Accuracy, making itslef the best-performing model for predicting proposal acceptance, for the given features RomanticGestureScore, CompatibilityScore, and DistanceKM.


### Question 4: How can we enhance the accuracy of a Random Forest model in predicting proposal acceptance using features like Income, Age, and DistanceKM? Adding what other features might further improve the model's performance, and how would the model's accuracy change as a result?

# Split the dataset into training and testing sets 

set.seed(123)
trainIndex <- createDataPartition(proposal_df$Response, p = 0.8, list = FALSE)
trainData <- proposal_df[trainIndex, ]
testData <- proposal_df[-trainIndex, ]

# Random Forest Model
random_forest_model <- randomForest(Response ~ Height + Income + Age + DistanceKM, data = trainData, ntree = 500, mtry = 2, importance = TRUE)
print(random_forest_model)
predicted_probabilities_rf <- predict(random_forest_model, proposal_df, type = "prob")[, 2]  # Probabilities for class 1
predicted_classes_rf <- predict(random_forest_model, proposal_df, type = "response")# Predicted classes
actual <- as.factor(proposal_df$Response)

# Confusion Matrix and metrics
confusion_rf <- confusionMatrix(as.factor(predicted_classes_rf), actual, positive = "1")
accuracy_rf <- confusion_rf$overall["Accuracy"]
precision_rf <- confusion_rf$byClass["Pos Pred Value"]  
recall_rf <- confusion_rf$byClass["Sensitivity"]        
f1_score_rf <- 2 * (precision_rf * recall_rf) / (precision_rf + recall_rf)
roc_curve_rf <- roc(actual, predicted_probabilities_rf, levels = c("0", "1"))
auc_value_rf <- auc(roc_curve_rf)


# Print Metrics
cat("Random Forest Metrics:\n")
cat("Accuracy:", round(accuracy_rf, 3), "\n")
cat("Precision:", round(precision_rf, 3), "\n")
cat("Recall:", round(recall_rf, 3), "\n")
cat("F1-Score:", round(f1_score_rf, 3), "\n")
cat("AUC:", round(auc_value_rf, 3), "\n")

# Plot ROC Curve
plot(roc_curve_rf, main = "ROC Curve for Random Forest", col = "blue", lwd = 2)
abline(a = 0, b = 1, col = "gray", lty = 2)
auc_text_rf <- paste("AUC =", round(auc_value_rf, 3)) # Add AUC value to the plot
text(0.6, 0.2, auc_text_rf, col = "blue", cex = 1.2)

```

We consider all variables here to obtain the Mean Decrease Gini Scores of each variable

```{r}

# Train a random forest model for comparision
rf_model <- randomForest(Response ~ ., data = proposal_df)

# View feature importance
importance(rf_model)
varImpPlot(rf_model) # Plot

```

The Final Random Forest Model

```{r}
# Updated Model

set.seed(123)
trainIndex <- createDataPartition(proposal_df$Response, p = 0.8, list = FALSE)
trainData <- proposal_df[trainIndex, ]
testData <- proposal_df[-trainIndex, ]

# Random Forest Model
random_forest_model <- randomForest(Response ~ Height + Income + Age + DistanceKM + CompatibilityScore + RomanticGestureScore, data = trainData, ntree = 500, mtry = 2, importance = TRUE)
print(random_forest_model)
predicted_probabilities_rf <- predict(random_forest_model, proposal_df, type = "prob")[, 2]  
predicted_classes_rf <- predict(random_forest_model, proposal_df, type = "response")
actual <- as.factor(proposal_df$Response)

# Confusion Matrix and metrics
confusion_rf <- confusionMatrix(as.factor(predicted_classes_rf), actual, positive = "1")
accuracy_rf <- confusion_rf$overall["Accuracy"]
precision_rf <- confusion_rf$byClass["Pos Pred Value"]  
recall_rf <- confusion_rf$byClass["Sensitivity"]        
f1_score_rf <- 2 * (precision_rf * recall_rf) / (precision_rf + recall_rf)
roc_curve_rf <- roc(actual, predicted_probabilities_rf, levels = c("0", "1"))
auc_value_rf <- auc(roc_curve_rf)

# Print Metrics
cat("Random Forest Metrics:\n")
cat("Accuracy:", round(accuracy_rf, 3), "\n")
cat("Precision:", round(precision_rf, 3), "\n")
cat("Recall:", round(recall_rf, 3), "\n")
cat("F1-Score:", round(f1_score_rf, 3), "\n")
cat("AUC:", round(auc_value_rf, 3), "\n")

# Plot ROC Curve
plot(roc_curve_rf, main = "ROC Curve for Random Forest", col = "blue", lwd = 2)
abline(a = 0, b = 1, col = "gray", lty = 2)
auc_text_rf <- paste("AUC =", round(auc_value_rf, 3)) 
text(0.6, 0.2, auc_text_rf, col = "blue", cex = 1.2)

# Variable Importance Plot
varImpPlot(random_forest_model, main = "Variable Importance - Random Forest")

```

Let's consider an example and try to predict the Response for given values.

```{r}

# Prediction model
subject <- data.frame(
  RomanticGestureScore = factor(10, levels = levels(proposal_df$RomanticGestureScore)), CompatibilityScore = factor(7, levels = levels(proposal_df$CompatibilityScore)), DistanceKM = 10, Height = 169, Income = 6000, Age = 23)

predicted_classes <- predict(random_forest_model, subject, type = "response")

predicted_probabilities <- predict(random_forest_model, subject, type = "prob")

cat("Predicted Class (0 or 1):\n")
print(predicted_classes)

The Model Predicts the outcome to be 'Yes'(1).

#### OBSERVATION:

```{r}

According to the study, adding more features to a Random Forest model can improve its ability to predict proposal acceptance. Initially, the model included factors such as Height, Income, Age, and DistanceKM, which led to an Out-Of-Bag (OOB) error rate of 24.1%. Key performance metrics, including an accuracy of 95.1%, precision of 94.4%, recall of 99.5%, and an AUC of 0.97, showcased the model's strong predictive capabilities. Nevertheless, the Mean Decrease Gini scores indicated that factors such as CompatibilityScore and RomanticGestureScore were significantly important and could enhance predictions further. 

Integrating CompatibilityScore and RomanticGestureScore into the model resulted in significant enhancements. The ultimate Random Forest model reached a reduced OOB error rate of 22.5% and improved performance metrics, with accuracy rising to 95.9%, precision at 95.9%, recall at 98.9%, and an AUC of 0.984. These results emphasize the significance of incorporating highly pertinent features into the model, which more accurately represent the nuances of proposal approval. The updated feature set highlights the significant influence of CompatibilityScore, underlining that interpersonal compatibility plays a crucial role in predictions, offering important insights for further model enhancements.

```

#### RESULT:
```{r}
```

# CONCLUSION

1. We have plotted a graph and observed the relation between acceptance and rejection ratio for individuals in different income categories. We have also conducted Pearson's Chi-squared test to see if there is any statistical difference in Response variable based on the individual's Income. We got a high p-value of 0.8 suggesting there is no significant association between Proposal and Income of an individual.

2. The linear regression model built using the features Age, Height, and RomanticGestureScore as predictors to determine the income of an individual has resulted in an effective model. This can be supported with the help of low R-squared value of 0.000851 and the high Mean Squared Error (MSE) of 1.7e+08. This means the chosen features do not explain much variation in income, and/or the model Linear regression is inefficient in making accurate predictions for this data or these features.

3. After building the three models—Logistic Regression, Decision Tree, and Random Forest and comparing them using evaluation metrics, it is evident that the Random Forest is the most suitable model for predicting proposal acceptance using features such as RomanticGestureScore, CompatibilityScore, and DistanceKM. The Random forest has better accuracy and AUC and good precision and F1-score compared to the other two models.

4. While trying to increase the efficiency of Random Forest model used in the 3rd SMART question, we found that physical features, such as including features such as Height, Income, Age, and DistanceKM can produce an even more efficient model compared to the relational attributes used. However, by adding CompatibilityScore and RomanticGestureScore, significant improvements were observed with an increase in accuracy and a decrease in OOB error rate.


# REFERENCES
```{r}
```



